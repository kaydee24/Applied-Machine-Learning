{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network_LAB8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8qyF5beDZ29"
      },
      "source": [
        "##This block is only for access of files using google drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#For accessing any file from google drive, first share it for public access. Copy its id from last part of its address. Then specify the two lines below.\n",
        "downloaded1 = drive.CreateFile({'id':\"1WdrjrDmyRlIvXhtSlwwcmqEwjg966qRp\"})   # replace the id with id of file you want to access\n",
        "downloaded2 = drive.CreateFile({'id':\"1vN5qL2Rpd4Fuw2yNe26zLEVt1GU5cYuu\"})\n",
        "downloaded1.GetContentFile('fashion-mnist_train.csv')        # replace the file name with your file\n",
        "downloaded2.GetContentFile('fashion-mnist_test.csv')        # replace the file name with your file\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhiP-xvIJDyu"
      },
      "source": [
        "Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n",
        "\n",
        "The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \"If it doesn't work on MNIST, it won't work at all\", they said. \"Well, if it does work on MNIST, it may still fail on others.\"\n",
        "\n",
        "Zalando seeks to replace the original MNIST dataset\n",
        "\n",
        "Content\n",
        "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.\n",
        "\n",
        "To locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix.\n",
        "For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below.\n",
        "\n",
        "\n",
        "Labels\n",
        "\n",
        "Each training and test example is assigned to one of the following labels:\n",
        "\n",
        "0 T-shirt/top\n",
        "\n",
        "1 Trouser\n",
        "\n",
        "2 Pullover\n",
        "\n",
        "3 Dress\n",
        "\n",
        "4 Coat\n",
        "\n",
        "5 Sandal\n",
        "\n",
        "6 Shirt\n",
        "\n",
        "7 Sneaker\n",
        "\n",
        "8 Bag\n",
        "\n",
        "9 Ankle boot\n",
        "\n",
        "\n",
        "**Assignment 8 (3 Marks)**\n",
        "1. Make a Neural network by using Tensorflow to find the image label with high accuracy. Report accuracy in both training and test sets. Input layer should have 784 units and output layer should have 10 units. (2)\n",
        "\n",
        "2. By having a single RELU unit in the output layer, try to find the label of the set using a regression approach. (1)\n",
        "\n",
        "The codes below are just to help you understand the data structure. Please take guidance from Assignement 6 on the way to proceed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EuRKxtQRmZE",
        "outputId": "bee3d769-cbbb-4dae-ed93-da4d4dd82c4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqq2HA-FEj22"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.layers import Dense, Input, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfeoNl0UEoki"
      },
      "source": [
        "fashion_Train = pd.read_csv('fashion-mnist_train.csv')\n",
        "fashion_Train = shuffle(fashion_Train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jhlN_J5E6-d"
      },
      "source": [
        "fashion_Test = pd.read_csv('fashion-mnist_test.csv')\n",
        "fashion_Test = shuffle(fashion_Test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vabby-2KFNm8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTvPUSUJFREt"
      },
      "source": [
        "fashion_Train_Y = np.array(fashion_Train['label']);\n",
        "fashion_Test_Y = np.array(fashion_Test['label']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bL-8HCtGa8M"
      },
      "source": [
        "fashion_Train_X = fashion_Train.drop(columns = 'label')\n",
        "fashion_Test_X = fashion_Test.drop(columns = 'label')\n",
        "\n",
        "fashion_Train_X=fashion_Train_X/255.0 #normalisation of data\n",
        "fashion_Test_X/=255.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw8gplCfHDK1"
      },
      "source": [
        "#fashion_Train_X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0OR9SCfFY99",
        "outputId": "b917cca8-c9df-497a-df86-e25e879c487b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "fashion_Train_X.iloc[[0]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>pixel39</th>\n",
              "      <th>pixel40</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel745</th>\n",
              "      <th>pixel746</th>\n",
              "      <th>pixel747</th>\n",
              "      <th>pixel748</th>\n",
              "      <th>pixel749</th>\n",
              "      <th>pixel750</th>\n",
              "      <th>pixel751</th>\n",
              "      <th>pixel752</th>\n",
              "      <th>pixel753</th>\n",
              "      <th>pixel754</th>\n",
              "      <th>pixel755</th>\n",
              "      <th>pixel756</th>\n",
              "      <th>pixel757</th>\n",
              "      <th>pixel758</th>\n",
              "      <th>pixel759</th>\n",
              "      <th>pixel760</th>\n",
              "      <th>pixel761</th>\n",
              "      <th>pixel762</th>\n",
              "      <th>pixel763</th>\n",
              "      <th>pixel764</th>\n",
              "      <th>pixel765</th>\n",
              "      <th>pixel766</th>\n",
              "      <th>pixel767</th>\n",
              "      <th>pixel768</th>\n",
              "      <th>pixel769</th>\n",
              "      <th>pixel770</th>\n",
              "      <th>pixel771</th>\n",
              "      <th>pixel772</th>\n",
              "      <th>pixel773</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "      <th>pixel784</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>45279</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.45098</td>\n",
              "      <td>0.129412</td>\n",
              "      <td>0.117647</td>\n",
              "      <td>0.058824</td>\n",
              "      <td>0.062745</td>\n",
              "      <td>0.101961</td>\n",
              "      <td>0.12549</td>\n",
              "      <td>0.313725</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.007843</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.160784</td>\n",
              "      <td>0.756863</td>\n",
              "      <td>0.827451</td>\n",
              "      <td>...</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.643137</td>\n",
              "      <td>0.615686</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.545098</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.345098</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.45098</td>\n",
              "      <td>0.501961</td>\n",
              "      <td>0.576471</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.619608</td>\n",
              "      <td>0.580392</td>\n",
              "      <td>0.560784</td>\n",
              "      <td>0.52549</td>\n",
              "      <td>0.521569</td>\n",
              "      <td>0.509804</td>\n",
              "      <td>0.223529</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003922</td>\n",
              "      <td>0.007843</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 784 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       pixel1  pixel2  pixel3  pixel4  ...  pixel781  pixel782  pixel783  pixel784\n",
              "45279     0.0     0.0     0.0     0.0  ...       0.0       0.0       0.0       0.0\n",
              "\n",
              "[1 rows x 784 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YSVmhGRPekr",
        "outputId": "16a000c3-d343-48a1-fb9d-e73d9cff2172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.imshow(np.array(fashion_Train_X.iloc[2]).reshape(28,28))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3966355b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faiL7c-EGaFa",
        "outputId": "dc27f5c6-f93b-4cb8-f29a-ef311c983f49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2873
        }
      },
      "source": [
        "np.array(fashion_Train_X.iloc[[0]]).reshape(28,28)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.45098039, 0.12941176, 0.11764706, 0.05882353, 0.0627451 ,\n",
              "        0.10196078, 0.1254902 , 0.31372549, 0.        , 0.        ,\n",
              "        0.        , 0.00784314, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.16078431,\n",
              "        0.75686275, 0.82745098, 0.52156863, 0.48627451, 0.57254902,\n",
              "        0.49019608, 0.68235294, 0.85098039, 0.14509804, 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.16470588, 0.35294118, 0.45490196, 0.42352941,\n",
              "        0.38431373, 0.61960784, 0.80784314, 0.83529412, 0.84705882,\n",
              "        0.83137255, 0.77254902, 0.43921569, 0.41568627, 0.4       ,\n",
              "        0.29019608, 0.09411765, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.24313725, 0.45490196, 0.41568627, 0.43529412, 0.4       ,\n",
              "        0.40392157, 0.30588235, 0.27058824, 0.34509804, 0.30588235,\n",
              "        0.25098039, 0.31372549, 0.36078431, 0.39607843, 0.45490196,\n",
              "        0.41568627, 0.48627451, 0.13333333, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.03137255,\n",
              "        0.42352941, 0.45490196, 0.38823529, 0.4       , 0.38039216,\n",
              "        0.37647059, 0.38431373, 0.41568627, 0.38431373, 0.37647059,\n",
              "        0.40392157, 0.40392157, 0.38823529, 0.43137255, 0.4       ,\n",
              "        0.41960784, 0.48627451, 0.43529412, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.25882353,\n",
              "        0.4745098 , 0.4745098 , 0.41176471, 0.35294118, 0.43137255,\n",
              "        0.5372549 , 0.43921569, 0.38431373, 0.43921569, 0.4745098 ,\n",
              "        0.46666667, 0.37647059, 0.49019608, 0.49019608, 0.38823529,\n",
              "        0.41568627, 0.48235294, 0.50980392, 0.10196078, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.38823529,\n",
              "        0.43921569, 0.49019608, 0.50588235, 0.3254902 , 0.45882353,\n",
              "        0.54901961, 0.43529412, 0.35294118, 0.51372549, 0.50196078,\n",
              "        0.4745098 , 0.52941176, 0.44705882, 0.56470588, 0.36862745,\n",
              "        0.44705882, 0.47058824, 0.48235294, 0.29411765, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.48235294,\n",
              "        0.41960784, 0.45882353, 0.57647059, 0.30980392, 0.38431373,\n",
              "        0.55686275, 0.35294118, 0.36470588, 0.43921569, 0.36862745,\n",
              "        0.39607843, 0.50588235, 0.45882353, 0.56078431, 0.31372549,\n",
              "        0.50588235, 0.47058824, 0.45490196, 0.38431373, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.4745098 ,\n",
              "        0.39607843, 0.38431373, 0.65490196, 0.37647059, 0.32156863,\n",
              "        0.5372549 , 0.46666667, 0.52156863, 0.46666667, 0.37647059,\n",
              "        0.44705882, 0.50196078, 0.55686275, 0.54117647, 0.32941176,\n",
              "        0.52941176, 0.45098039, 0.44705882, 0.43137255, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.02745098, 0.45882353,\n",
              "        0.38039216, 0.30980392, 0.69019608, 0.4745098 , 0.29803922,\n",
              "        0.39607843, 0.2745098 , 0.36862745, 0.30980392, 0.20784314,\n",
              "        0.29019608, 0.4       , 0.29411765, 0.43921569, 0.35294118,\n",
              "        0.57647059, 0.40392157, 0.41176471, 0.49411765, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.07843137, 0.46666667,\n",
              "        0.39607843, 0.34509804, 0.7254902 , 0.60784314, 0.21960784,\n",
              "        0.7254902 , 0.83137255, 0.80784314, 0.85098039, 0.94117647,\n",
              "        0.77647059, 0.74117647, 0.85882353, 0.56078431, 0.34901961,\n",
              "        0.63137255, 0.38431373, 0.41568627, 0.52941176, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.11764706, 0.45490196,\n",
              "        0.38039216, 0.36078431, 0.57647059, 0.58431373, 0.23921569,\n",
              "        0.83137255, 0.81568627, 0.74509804, 0.74117647, 0.77254902,\n",
              "        0.76078431, 0.7372549 , 0.90588235, 0.78039216, 0.44705882,\n",
              "        0.69019608, 0.34117647, 0.41960784, 0.52941176, 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.16470588, 0.44705882,\n",
              "        0.34901961, 0.37647059, 0.4745098 , 0.57254902, 0.31372549,\n",
              "        0.80392157, 0.89803922, 0.80392157, 0.62745098, 0.83137255,\n",
              "        0.85882353, 0.85882353, 0.96470588, 0.79215686, 0.47058824,\n",
              "        0.61568627, 0.3254902 , 0.38039216, 0.52941176, 0.08235294,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.17254902, 0.45882353,\n",
              "        0.34117647, 0.38039216, 0.43529412, 0.49411765, 0.38823529,\n",
              "        0.41960784, 0.36078431, 0.36470588, 0.36862745, 0.36470588,\n",
              "        0.31372549, 0.36470588, 0.44705882, 0.44705882, 0.56078431,\n",
              "        0.49411765, 0.38431373, 0.36470588, 0.52156863, 0.14901961,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.18823529, 0.44705882,\n",
              "        0.29803922, 0.43529412, 0.36862745, 0.41568627, 0.43137255,\n",
              "        0.52941176, 0.43137255, 0.49019608, 0.43137255, 0.34117647,\n",
              "        0.39607843, 0.50980392, 0.43921569, 0.52156863, 0.52156863,\n",
              "        0.45098039, 0.41176471, 0.35294118, 0.50588235, 0.20784314,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.25490196, 0.42352941,\n",
              "        0.31372549, 0.45098039, 0.27058824, 0.36078431, 0.45882353,\n",
              "        0.48235294, 0.39607843, 0.50196078, 0.43137255, 0.34901961,\n",
              "        0.41568627, 0.49411765, 0.43529412, 0.52941176, 0.38039216,\n",
              "        0.39607843, 0.41568627, 0.34117647, 0.49411765, 0.25490196,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.29019608, 0.4       ,\n",
              "        0.33333333, 0.43137255, 0.21960784, 0.38431373, 0.45098039,\n",
              "        0.49019608, 0.45490196, 0.50588235, 0.43921569, 0.45098039,\n",
              "        0.42352941, 0.50588235, 0.44705882, 0.56078431, 0.4       ,\n",
              "        0.36470588, 0.43137255, 0.36470588, 0.48627451, 0.28627451,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.31372549, 0.38823529,\n",
              "        0.32941176, 0.43921569, 0.21568627, 0.36078431, 0.44705882,\n",
              "        0.48235294, 0.49411765, 0.46666667, 0.48627451, 0.44705882,\n",
              "        0.49411765, 0.4745098 , 0.5254902 , 0.5372549 , 0.48627451,\n",
              "        0.42352941, 0.39607843, 0.37647059, 0.45098039, 0.30588235,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.34509804, 0.38431373,\n",
              "        0.32941176, 0.43921569, 0.2       , 0.39607843, 0.40392157,\n",
              "        0.47058824, 0.45490196, 0.37647059, 0.42352941, 0.48627451,\n",
              "        0.45098039, 0.36470588, 0.50588235, 0.48235294, 0.49411765,\n",
              "        0.43921569, 0.38039216, 0.36078431, 0.43921569, 0.33333333,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.36862745, 0.38823529,\n",
              "        0.3254902 , 0.38431373, 0.20392157, 0.45882353, 0.41568627,\n",
              "        0.41568627, 0.39607843, 0.4       , 0.36862745, 0.38823529,\n",
              "        0.38431373, 0.41568627, 0.39607843, 0.39607843, 0.49019608,\n",
              "        0.29411765, 0.41960784, 0.36862745, 0.46666667, 0.33333333,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.34901961, 0.36470588,\n",
              "        0.36470588, 0.38431373, 0.25882353, 0.47058824, 0.41568627,\n",
              "        0.45882353, 0.43921569, 0.43137255, 0.44705882, 0.43921569,\n",
              "        0.44705882, 0.46666667, 0.49019608, 0.46666667, 0.52941176,\n",
              "        0.27058824, 0.43529412, 0.38823529, 0.4745098 , 0.32156863,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.35294118, 0.35294118,\n",
              "        0.36862745, 0.36470588, 0.30588235, 0.45490196, 0.4       ,\n",
              "        0.43921569, 0.46666667, 0.43529412, 0.43529412, 0.45098039,\n",
              "        0.49019608, 0.48235294, 0.47058824, 0.4745098 , 0.54901961,\n",
              "        0.19215686, 0.40392157, 0.38039216, 0.44705882, 0.30588235,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.34509804, 0.39607843,\n",
              "        0.39607843, 0.33333333, 0.33333333, 0.42352941, 0.38823529,\n",
              "        0.43137255, 0.43137255, 0.4745098 , 0.45882353, 0.43529412,\n",
              "        0.49411765, 0.48627451, 0.45098039, 0.45490196, 0.56078431,\n",
              "        0.16470588, 0.43137255, 0.38823529, 0.48235294, 0.29019608,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.18431373, 0.21568627,\n",
              "        0.18039216, 0.19215686, 0.33333333, 0.45098039, 0.38823529,\n",
              "        0.42352941, 0.43529412, 0.44705882, 0.45098039, 0.48235294,\n",
              "        0.49019608, 0.45490196, 0.45882353, 0.43137255, 0.58039216,\n",
              "        0.16470588, 0.30588235, 0.36078431, 0.41568627, 0.14901961,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.61568627, 0.80784314,\n",
              "        0.82352941, 0.65490196, 0.2745098 , 0.50196078, 0.45882353,\n",
              "        0.4745098 , 0.45098039, 0.45098039, 0.45490196, 0.49019608,\n",
              "        0.48627451, 0.45490196, 0.48627451, 0.43529412, 0.57647059,\n",
              "        0.18431373, 0.88627451, 0.84705882, 0.89411765, 0.45490196,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.27843137, 0.35294118,\n",
              "        0.47058824, 0.32941176, 0.18039216, 0.29411765, 0.25490196,\n",
              "        0.29411765, 0.3254902 , 0.38431373, 0.41176471, 0.41960784,\n",
              "        0.40392157, 0.41960784, 0.37647059, 0.35294118, 0.41960784,\n",
              "        0.12941176, 0.16862745, 0.21568627, 0.29803922, 0.10980392,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 1.        , 0.69803922, 0.74117647,\n",
              "        0.69803922, 0.61568627, 0.59215686, 0.57647059, 0.57647059,\n",
              "        0.58039216, 0.6       , 0.64313725, 0.61568627, 0.70588235,\n",
              "        0.54509804, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
              "        0.        , 0.        , 0.34509804, 0.47058824, 0.45098039,\n",
              "        0.50196078, 0.57647059, 0.66666667, 0.66666667, 0.61960784,\n",
              "        0.58039216, 0.56078431, 0.5254902 , 0.52156863, 0.50980392,\n",
              "        0.22352941, 0.        , 0.00392157, 0.00784314, 0.        ,\n",
              "        0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFZfBAu3H6pl",
        "outputId": "79f102df-5b52-4244-b8d9-1722a241d445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "image_Index = 49; # You may change this index and see the images below and values in next two blocks\n",
        "plt.imshow(np.array(fashion_Train_X.iloc[[image_Index]]).reshape(28,28), cmap = 'gray')\n",
        "plt.title(fashion_Train_Y[image_Index])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, '0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpoZ9aypIIEr",
        "outputId": "d56c185e-cde7-46ce-b6e9-3d5411fec547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "fashion_Train_Y[image_Index]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGxnjGLgIM1L",
        "outputId": "e77414a0-a563-4806-e652-7a051b33ddb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "#Question 1\n",
        "input_img=Input(shape=(784,))                      #Input layer with 784 neurons  \n",
        "x=Dense(300,activation=tf.nn.relu)(input_img)   #1st hidden layer with 300 neurons\n",
        "x=Dense(100,activation=tf.nn.relu)(x)           #2nd hidden layer with 100 neurons\n",
        "output=Dense(10,activation=tf.nn.sigmoid)(x)       #Output layer with 10 neurons \n",
        "fashion_model=Model(input_img,output)              #Defining the Model\n",
        "fashion_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               30100     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 266,610\n",
            "Trainable params: 266,610\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxyBHcD1KrOZ",
        "outputId": "6d183248-ee08-43db-8cb4-aeab4e0d0ebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1394
        }
      },
      "source": [
        "fashion_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])  #Compiling the Model\n",
        "fashion_model.fit(fashion_Train_X,fashion_Train_Y,epochs=40,validation_data=(fashion_Test_X,fashion_Test_Y))   #Fitting the model and testing its accuracy on the test set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5040 - accuracy: 0.8222 - val_loss: 0.3759 - val_accuracy: 0.8668\n",
            "Epoch 2/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3733 - accuracy: 0.8648 - val_loss: 0.3574 - val_accuracy: 0.8719\n",
            "Epoch 3/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3363 - accuracy: 0.8766 - val_loss: 0.3307 - val_accuracy: 0.8806\n",
            "Epoch 4/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3109 - accuracy: 0.8848 - val_loss: 0.3220 - val_accuracy: 0.8836\n",
            "Epoch 5/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2946 - accuracy: 0.8922 - val_loss: 0.3264 - val_accuracy: 0.8775\n",
            "Epoch 6/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2760 - accuracy: 0.8962 - val_loss: 0.3201 - val_accuracy: 0.8818\n",
            "Epoch 7/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2643 - accuracy: 0.9003 - val_loss: 0.3088 - val_accuracy: 0.8886\n",
            "Epoch 8/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2534 - accuracy: 0.9047 - val_loss: 0.3294 - val_accuracy: 0.8798\n",
            "Epoch 9/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2417 - accuracy: 0.9083 - val_loss: 0.2955 - val_accuracy: 0.8935\n",
            "Epoch 10/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2313 - accuracy: 0.9121 - val_loss: 0.3194 - val_accuracy: 0.8908\n",
            "Epoch 11/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2237 - accuracy: 0.9150 - val_loss: 0.3059 - val_accuracy: 0.8934\n",
            "Epoch 12/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2158 - accuracy: 0.9181 - val_loss: 0.3058 - val_accuracy: 0.8946\n",
            "Epoch 13/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2079 - accuracy: 0.9208 - val_loss: 0.3050 - val_accuracy: 0.8953\n",
            "Epoch 14/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2000 - accuracy: 0.9222 - val_loss: 0.3420 - val_accuracy: 0.8848\n",
            "Epoch 15/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1948 - accuracy: 0.9251 - val_loss: 0.3324 - val_accuracy: 0.8916\n",
            "Epoch 16/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1888 - accuracy: 0.9274 - val_loss: 0.3099 - val_accuracy: 0.8950\n",
            "Epoch 17/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1820 - accuracy: 0.9301 - val_loss: 0.3307 - val_accuracy: 0.8929\n",
            "Epoch 18/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1780 - accuracy: 0.9320 - val_loss: 0.3356 - val_accuracy: 0.8961\n",
            "Epoch 19/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1725 - accuracy: 0.9334 - val_loss: 0.3416 - val_accuracy: 0.8964\n",
            "Epoch 20/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1678 - accuracy: 0.9353 - val_loss: 0.3457 - val_accuracy: 0.8972\n",
            "Epoch 21/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1642 - accuracy: 0.9366 - val_loss: 0.3419 - val_accuracy: 0.8973\n",
            "Epoch 22/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1612 - accuracy: 0.9383 - val_loss: 0.3459 - val_accuracy: 0.8993\n",
            "Epoch 23/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1569 - accuracy: 0.9381 - val_loss: 0.3632 - val_accuracy: 0.8940\n",
            "Epoch 24/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1484 - accuracy: 0.9426 - val_loss: 0.3575 - val_accuracy: 0.9000\n",
            "Epoch 25/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1515 - accuracy: 0.9407 - val_loss: 0.3764 - val_accuracy: 0.8989\n",
            "Epoch 26/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1425 - accuracy: 0.9439 - val_loss: 0.3710 - val_accuracy: 0.8967\n",
            "Epoch 27/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1424 - accuracy: 0.9456 - val_loss: 0.3619 - val_accuracy: 0.8980\n",
            "Epoch 28/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1362 - accuracy: 0.9463 - val_loss: 0.3759 - val_accuracy: 0.9009\n",
            "Epoch 29/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1356 - accuracy: 0.9476 - val_loss: 0.3598 - val_accuracy: 0.9003\n",
            "Epoch 30/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1304 - accuracy: 0.9481 - val_loss: 0.3707 - val_accuracy: 0.9020\n",
            "Epoch 31/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1303 - accuracy: 0.9484 - val_loss: 0.3803 - val_accuracy: 0.8987\n",
            "Epoch 32/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1259 - accuracy: 0.9509 - val_loss: 0.3974 - val_accuracy: 0.8983\n",
            "Epoch 33/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1256 - accuracy: 0.9510 - val_loss: 0.4109 - val_accuracy: 0.9008\n",
            "Epoch 34/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1201 - accuracy: 0.9525 - val_loss: 0.3955 - val_accuracy: 0.8995\n",
            "Epoch 35/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1189 - accuracy: 0.9531 - val_loss: 0.4150 - val_accuracy: 0.8998\n",
            "Epoch 36/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1169 - accuracy: 0.9536 - val_loss: 0.4121 - val_accuracy: 0.8987\n",
            "Epoch 37/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1151 - accuracy: 0.9540 - val_loss: 0.4288 - val_accuracy: 0.9007\n",
            "Epoch 38/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1110 - accuracy: 0.9558 - val_loss: 0.4453 - val_accuracy: 0.8969\n",
            "Epoch 39/40\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1135 - accuracy: 0.9555 - val_loss: 0.4484 - val_accuracy: 0.8967\n",
            "Epoch 40/40\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1096 - accuracy: 0.9560 - val_loss: 0.3821 - val_accuracy: 0.9023\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f39631efbe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXgwtDh3cMRk"
      },
      "source": [
        "**We are able to get 95.60% accuracy on the Test set and 90.23% accuracy on Training set.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6G-P2uWct66",
        "outputId": "82a1cce0-bb59-4aa3-cf0e-47c169322de0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6854
        }
      },
      "source": [
        "#Question 2\n",
        "input_vector=Input(shape=(784,))                      #Input layer with 784 neurons  \n",
        "x=Dense(128,activation=tf.nn.relu)(input_vector)   #1st hidden layer with 128 neurons\n",
        "x=Dense(128,activation=tf.nn.relu)(x)                 #2nd hidden layer with 128 neurons\n",
        "x=Dense(64,activation=tf.nn.relu)(x)               #3rd hidden layer with 64 neurons\n",
        "output_relu=Dense(1,activation=tf.nn.relu)(x)        #Output layer with 1 neuron \n",
        "fashion_model_relu=Model(input_vector,output_relu)              #Defining the Model\n",
        "fashion_model_relu.compile(optimizer=\"adam\", loss=\"mean_squared_error\",metrics=[\"mse\",\"accuracy\"])  #Compiling the Model\n",
        "fashion_model_relu.fit(fashion_Train_X,fashion_Train_Y,epochs=200,validation_data=(fashion_Test_X,fashion_Test_Y))   #Fitting the model and testing its accuracy on the test set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.9408 - mse: 1.9408 - accuracy: 0.1312 - val_loss: 1.6409 - val_mse: 1.6409 - val_accuracy: 0.1442\n",
            "Epoch 2/200\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 1.4089 - mse: 1.4089 - accuracy: 0.1408 - val_loss: 1.4272 - val_mse: 1.4272 - val_accuracy: 0.1536\n",
            "Epoch 3/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.2632 - mse: 1.2632 - accuracy: 0.1413 - val_loss: 1.3042 - val_mse: 1.3042 - val_accuracy: 0.1411\n",
            "Epoch 4/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.1685 - mse: 1.1685 - accuracy: 0.1474 - val_loss: 1.2484 - val_mse: 1.2484 - val_accuracy: 0.1607\n",
            "Epoch 5/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.1184 - mse: 1.1184 - accuracy: 0.1477 - val_loss: 1.2375 - val_mse: 1.2375 - val_accuracy: 0.1299\n",
            "Epoch 6/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.0789 - mse: 1.0789 - accuracy: 0.1466 - val_loss: 1.2205 - val_mse: 1.2205 - val_accuracy: 0.1490\n",
            "Epoch 7/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.0348 - mse: 1.0348 - accuracy: 0.1482 - val_loss: 1.1956 - val_mse: 1.1956 - val_accuracy: 0.1527\n",
            "Epoch 8/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.0021 - mse: 1.0021 - accuracy: 0.1512 - val_loss: 1.2211 - val_mse: 1.2211 - val_accuracy: 0.1425\n",
            "Epoch 9/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.9773 - mse: 0.9773 - accuracy: 0.1517 - val_loss: 1.2525 - val_mse: 1.2525 - val_accuracy: 0.1666\n",
            "Epoch 10/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.9579 - mse: 0.9579 - accuracy: 0.1512 - val_loss: 1.1118 - val_mse: 1.1118 - val_accuracy: 0.1327\n",
            "Epoch 11/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.9315 - mse: 0.9315 - accuracy: 0.1533 - val_loss: 1.1536 - val_mse: 1.1536 - val_accuracy: 0.1526\n",
            "Epoch 12/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.9081 - mse: 0.9081 - accuracy: 0.1543 - val_loss: 1.1294 - val_mse: 1.1294 - val_accuracy: 0.1478\n",
            "Epoch 13/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8868 - mse: 0.8868 - accuracy: 0.1541 - val_loss: 1.0935 - val_mse: 1.0935 - val_accuracy: 0.1460\n",
            "Epoch 14/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8655 - mse: 0.8655 - accuracy: 0.1567 - val_loss: 1.1053 - val_mse: 1.1053 - val_accuracy: 0.1465\n",
            "Epoch 15/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8495 - mse: 0.8495 - accuracy: 0.1561 - val_loss: 1.1698 - val_mse: 1.1698 - val_accuracy: 0.1587\n",
            "Epoch 16/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8361 - mse: 0.8361 - accuracy: 0.1578 - val_loss: 1.1047 - val_mse: 1.1047 - val_accuracy: 0.1543\n",
            "Epoch 17/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8091 - mse: 0.8091 - accuracy: 0.1576 - val_loss: 1.0838 - val_mse: 1.0838 - val_accuracy: 0.1602\n",
            "Epoch 18/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.7951 - mse: 0.7951 - accuracy: 0.1591 - val_loss: 1.1136 - val_mse: 1.1136 - val_accuracy: 0.1508\n",
            "Epoch 19/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.7713 - mse: 0.7713 - accuracy: 0.1615 - val_loss: 1.0643 - val_mse: 1.0643 - val_accuracy: 0.1567\n",
            "Epoch 20/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.7570 - mse: 0.7570 - accuracy: 0.1616 - val_loss: 1.0629 - val_mse: 1.0629 - val_accuracy: 0.1574\n",
            "Epoch 21/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.7373 - mse: 0.7373 - accuracy: 0.1634 - val_loss: 1.1013 - val_mse: 1.1013 - val_accuracy: 0.1528\n",
            "Epoch 22/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.7281 - mse: 0.7281 - accuracy: 0.1621 - val_loss: 1.0563 - val_mse: 1.0563 - val_accuracy: 0.1607\n",
            "Epoch 23/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.7195 - mse: 0.7195 - accuracy: 0.1622 - val_loss: 1.0952 - val_mse: 1.0952 - val_accuracy: 0.1646\n",
            "Epoch 24/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.7020 - mse: 0.7020 - accuracy: 0.1652 - val_loss: 1.1754 - val_mse: 1.1754 - val_accuracy: 0.1478\n",
            "Epoch 25/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6918 - mse: 0.6918 - accuracy: 0.1640 - val_loss: 1.1112 - val_mse: 1.1112 - val_accuracy: 0.1711\n",
            "Epoch 26/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6745 - mse: 0.6745 - accuracy: 0.1650 - val_loss: 1.1193 - val_mse: 1.1193 - val_accuracy: 0.1563\n",
            "Epoch 27/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6734 - mse: 0.6734 - accuracy: 0.1664 - val_loss: 1.1044 - val_mse: 1.1044 - val_accuracy: 0.1597\n",
            "Epoch 28/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6518 - mse: 0.6518 - accuracy: 0.1662 - val_loss: 1.2339 - val_mse: 1.2339 - val_accuracy: 0.1634\n",
            "Epoch 29/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6467 - mse: 0.6467 - accuracy: 0.1678 - val_loss: 1.1038 - val_mse: 1.1038 - val_accuracy: 0.1532\n",
            "Epoch 30/200\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.6319 - mse: 0.6319 - accuracy: 0.1679 - val_loss: 1.0386 - val_mse: 1.0386 - val_accuracy: 0.1549\n",
            "Epoch 31/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6393 - mse: 0.6393 - accuracy: 0.1691 - val_loss: 1.1623 - val_mse: 1.1623 - val_accuracy: 0.1641\n",
            "Epoch 32/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6042 - mse: 0.6042 - accuracy: 0.1699 - val_loss: 1.0825 - val_mse: 1.0825 - val_accuracy: 0.1635\n",
            "Epoch 33/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6237 - mse: 0.6237 - accuracy: 0.1707 - val_loss: 1.1176 - val_mse: 1.1176 - val_accuracy: 0.1652\n",
            "Epoch 34/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5943 - mse: 0.5943 - accuracy: 0.1699 - val_loss: 1.1358 - val_mse: 1.1358 - val_accuracy: 0.1658\n",
            "Epoch 35/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5912 - mse: 0.5912 - accuracy: 0.1705 - val_loss: 1.1274 - val_mse: 1.1274 - val_accuracy: 0.1574\n",
            "Epoch 36/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5841 - mse: 0.5841 - accuracy: 0.1716 - val_loss: 1.0895 - val_mse: 1.0895 - val_accuracy: 0.1667\n",
            "Epoch 37/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5738 - mse: 0.5738 - accuracy: 0.1731 - val_loss: 1.0874 - val_mse: 1.0874 - val_accuracy: 0.1553\n",
            "Epoch 38/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5576 - mse: 0.5576 - accuracy: 0.1732 - val_loss: 1.1223 - val_mse: 1.1223 - val_accuracy: 0.1595\n",
            "Epoch 39/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5541 - mse: 0.5541 - accuracy: 0.1727 - val_loss: 1.1813 - val_mse: 1.1813 - val_accuracy: 0.1679\n",
            "Epoch 40/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5466 - mse: 0.5466 - accuracy: 0.1718 - val_loss: 1.1521 - val_mse: 1.1521 - val_accuracy: 0.1599\n",
            "Epoch 41/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5360 - mse: 0.5360 - accuracy: 0.1735 - val_loss: 1.0868 - val_mse: 1.0868 - val_accuracy: 0.1630\n",
            "Epoch 42/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5255 - mse: 0.5255 - accuracy: 0.1738 - val_loss: 1.1699 - val_mse: 1.1699 - val_accuracy: 0.1647\n",
            "Epoch 43/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5250 - mse: 0.5250 - accuracy: 0.1748 - val_loss: 1.1172 - val_mse: 1.1172 - val_accuracy: 0.1584\n",
            "Epoch 44/200\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.5270 - mse: 0.5270 - accuracy: 0.1771 - val_loss: 1.1113 - val_mse: 1.1113 - val_accuracy: 0.1710\n",
            "Epoch 45/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5103 - mse: 0.5103 - accuracy: 0.1769 - val_loss: 1.2092 - val_mse: 1.2092 - val_accuracy: 0.1720\n",
            "Epoch 46/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.5137 - mse: 0.5137 - accuracy: 0.1750 - val_loss: 1.1050 - val_mse: 1.1050 - val_accuracy: 0.1701\n",
            "Epoch 47/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4972 - mse: 0.4972 - accuracy: 0.1765 - val_loss: 1.0923 - val_mse: 1.0923 - val_accuracy: 0.1616\n",
            "Epoch 48/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4934 - mse: 0.4934 - accuracy: 0.1759 - val_loss: 1.1379 - val_mse: 1.1379 - val_accuracy: 0.1714\n",
            "Epoch 49/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4855 - mse: 0.4855 - accuracy: 0.1773 - val_loss: 1.1524 - val_mse: 1.1524 - val_accuracy: 0.1734\n",
            "Epoch 50/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4824 - mse: 0.4824 - accuracy: 0.1776 - val_loss: 1.1327 - val_mse: 1.1327 - val_accuracy: 0.1651\n",
            "Epoch 51/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4972 - mse: 0.4972 - accuracy: 0.1769 - val_loss: 1.2269 - val_mse: 1.2269 - val_accuracy: 0.1593\n",
            "Epoch 52/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4666 - mse: 0.4666 - accuracy: 0.1785 - val_loss: 1.1273 - val_mse: 1.1273 - val_accuracy: 0.1703\n",
            "Epoch 53/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4804 - mse: 0.4804 - accuracy: 0.1779 - val_loss: 1.1354 - val_mse: 1.1354 - val_accuracy: 0.1690\n",
            "Epoch 54/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4524 - mse: 0.4524 - accuracy: 0.1783 - val_loss: 1.3231 - val_mse: 1.3231 - val_accuracy: 0.1517\n",
            "Epoch 55/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4464 - mse: 0.4464 - accuracy: 0.1793 - val_loss: 1.1718 - val_mse: 1.1718 - val_accuracy: 0.1635\n",
            "Epoch 56/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4450 - mse: 0.4450 - accuracy: 0.1785 - val_loss: 1.1713 - val_mse: 1.1713 - val_accuracy: 0.1662\n",
            "Epoch 57/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4516 - mse: 0.4516 - accuracy: 0.1801 - val_loss: 1.1376 - val_mse: 1.1376 - val_accuracy: 0.1728\n",
            "Epoch 58/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4330 - mse: 0.4330 - accuracy: 0.1812 - val_loss: 1.1431 - val_mse: 1.1431 - val_accuracy: 0.1681\n",
            "Epoch 59/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4297 - mse: 0.4297 - accuracy: 0.1826 - val_loss: 1.1725 - val_mse: 1.1725 - val_accuracy: 0.1686\n",
            "Epoch 60/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4293 - mse: 0.4293 - accuracy: 0.1803 - val_loss: 1.2073 - val_mse: 1.2073 - val_accuracy: 0.1719\n",
            "Epoch 61/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4195 - mse: 0.4195 - accuracy: 0.1812 - val_loss: 1.1698 - val_mse: 1.1698 - val_accuracy: 0.1730\n",
            "Epoch 62/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4256 - mse: 0.4256 - accuracy: 0.1822 - val_loss: 1.2006 - val_mse: 1.2006 - val_accuracy: 0.1644\n",
            "Epoch 63/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4130 - mse: 0.4130 - accuracy: 0.1809 - val_loss: 1.1621 - val_mse: 1.1621 - val_accuracy: 0.1679\n",
            "Epoch 64/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4156 - mse: 0.4156 - accuracy: 0.1830 - val_loss: 1.1659 - val_mse: 1.1659 - val_accuracy: 0.1685\n",
            "Epoch 65/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4190 - mse: 0.4190 - accuracy: 0.1825 - val_loss: 1.1945 - val_mse: 1.1945 - val_accuracy: 0.1731\n",
            "Epoch 66/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3889 - mse: 0.3889 - accuracy: 0.1830 - val_loss: 1.1634 - val_mse: 1.1634 - val_accuracy: 0.1722\n",
            "Epoch 67/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4046 - mse: 0.4046 - accuracy: 0.1833 - val_loss: 1.1994 - val_mse: 1.1994 - val_accuracy: 0.1725\n",
            "Epoch 68/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4098 - mse: 0.4098 - accuracy: 0.1823 - val_loss: 1.2134 - val_mse: 1.2134 - val_accuracy: 0.1684\n",
            "Epoch 69/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3829 - mse: 0.3829 - accuracy: 0.1840 - val_loss: 1.2159 - val_mse: 1.2159 - val_accuracy: 0.1731\n",
            "Epoch 70/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3847 - mse: 0.3847 - accuracy: 0.1847 - val_loss: 1.3734 - val_mse: 1.3734 - val_accuracy: 0.1622\n",
            "Epoch 71/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3861 - mse: 0.3861 - accuracy: 0.1840 - val_loss: 1.2080 - val_mse: 1.2080 - val_accuracy: 0.1682\n",
            "Epoch 72/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3773 - mse: 0.3773 - accuracy: 0.1841 - val_loss: 1.1871 - val_mse: 1.1871 - val_accuracy: 0.1730\n",
            "Epoch 73/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3882 - mse: 0.3882 - accuracy: 0.1836 - val_loss: 1.1879 - val_mse: 1.1879 - val_accuracy: 0.1752\n",
            "Epoch 74/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3795 - mse: 0.3795 - accuracy: 0.1846 - val_loss: 1.2011 - val_mse: 1.2011 - val_accuracy: 0.1696\n",
            "Epoch 75/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3571 - mse: 0.3571 - accuracy: 0.1856 - val_loss: 1.2149 - val_mse: 1.2149 - val_accuracy: 0.1715\n",
            "Epoch 76/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3673 - mse: 0.3673 - accuracy: 0.1851 - val_loss: 1.2704 - val_mse: 1.2704 - val_accuracy: 0.1671\n",
            "Epoch 77/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3580 - mse: 0.3580 - accuracy: 0.1857 - val_loss: 1.2166 - val_mse: 1.2166 - val_accuracy: 0.1705\n",
            "Epoch 78/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3598 - mse: 0.3598 - accuracy: 0.1858 - val_loss: 1.2324 - val_mse: 1.2324 - val_accuracy: 0.1694\n",
            "Epoch 79/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3595 - mse: 0.3595 - accuracy: 0.1863 - val_loss: 1.2715 - val_mse: 1.2715 - val_accuracy: 0.1765\n",
            "Epoch 80/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3511 - mse: 0.3511 - accuracy: 0.1871 - val_loss: 1.2137 - val_mse: 1.2137 - val_accuracy: 0.1745\n",
            "Epoch 81/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3516 - mse: 0.3516 - accuracy: 0.1862 - val_loss: 1.1969 - val_mse: 1.1969 - val_accuracy: 0.1695\n",
            "Epoch 82/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3406 - mse: 0.3406 - accuracy: 0.1872 - val_loss: 1.2962 - val_mse: 1.2962 - val_accuracy: 0.1694\n",
            "Epoch 83/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3433 - mse: 0.3433 - accuracy: 0.1865 - val_loss: 1.1988 - val_mse: 1.1988 - val_accuracy: 0.1786\n",
            "Epoch 84/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3383 - mse: 0.3383 - accuracy: 0.1874 - val_loss: 1.2314 - val_mse: 1.2314 - val_accuracy: 0.1771\n",
            "Epoch 85/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3431 - mse: 0.3431 - accuracy: 0.1870 - val_loss: 1.2301 - val_mse: 1.2301 - val_accuracy: 0.1710\n",
            "Epoch 86/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3324 - mse: 0.3324 - accuracy: 0.1870 - val_loss: 1.2076 - val_mse: 1.2076 - val_accuracy: 0.1729\n",
            "Epoch 87/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3322 - mse: 0.3322 - accuracy: 0.1874 - val_loss: 1.2413 - val_mse: 1.2413 - val_accuracy: 0.1705\n",
            "Epoch 88/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3248 - mse: 0.3248 - accuracy: 0.1876 - val_loss: 1.1618 - val_mse: 1.1618 - val_accuracy: 0.1715\n",
            "Epoch 89/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3237 - mse: 0.3237 - accuracy: 0.1878 - val_loss: 1.2316 - val_mse: 1.2316 - val_accuracy: 0.1698\n",
            "Epoch 90/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3349 - mse: 0.3349 - accuracy: 0.1876 - val_loss: 1.1946 - val_mse: 1.1946 - val_accuracy: 0.1722\n",
            "Epoch 91/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3304 - mse: 0.3304 - accuracy: 0.1886 - val_loss: 1.1756 - val_mse: 1.1756 - val_accuracy: 0.1733\n",
            "Epoch 92/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3252 - mse: 0.3252 - accuracy: 0.1881 - val_loss: 1.2390 - val_mse: 1.2390 - val_accuracy: 0.1744\n",
            "Epoch 93/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.3137 - mse: 0.3137 - accuracy: 0.1878 - val_loss: 1.2350 - val_mse: 1.2350 - val_accuracy: 0.1758\n",
            "Epoch 94/200\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.3122 - mse: 0.3122 - accuracy: 0.1887 - val_loss: 1.2323 - val_mse: 1.2323 - val_accuracy: 0.1733\n",
            "Epoch 95/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3253 - mse: 0.3253 - accuracy: 0.1881 - val_loss: 1.2032 - val_mse: 1.2032 - val_accuracy: 0.1757\n",
            "Epoch 96/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3010 - mse: 0.3010 - accuracy: 0.1880 - val_loss: 1.2041 - val_mse: 1.2041 - val_accuracy: 0.1749\n",
            "Epoch 97/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3056 - mse: 0.3056 - accuracy: 0.1891 - val_loss: 1.2737 - val_mse: 1.2737 - val_accuracy: 0.1799\n",
            "Epoch 98/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3085 - mse: 0.3085 - accuracy: 0.1884 - val_loss: 1.2106 - val_mse: 1.2106 - val_accuracy: 0.1704\n",
            "Epoch 99/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2997 - mse: 0.2997 - accuracy: 0.1890 - val_loss: 1.2207 - val_mse: 1.2207 - val_accuracy: 0.1769\n",
            "Epoch 100/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3053 - mse: 0.3053 - accuracy: 0.1883 - val_loss: 1.2158 - val_mse: 1.2158 - val_accuracy: 0.1747\n",
            "Epoch 101/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3037 - mse: 0.3037 - accuracy: 0.1892 - val_loss: 1.1870 - val_mse: 1.1870 - val_accuracy: 0.1725\n",
            "Epoch 102/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2907 - mse: 0.2907 - accuracy: 0.1897 - val_loss: 1.2043 - val_mse: 1.2043 - val_accuracy: 0.1786\n",
            "Epoch 103/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2903 - mse: 0.2903 - accuracy: 0.1896 - val_loss: 1.1904 - val_mse: 1.1904 - val_accuracy: 0.1714\n",
            "Epoch 104/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2832 - mse: 0.2832 - accuracy: 0.1897 - val_loss: 1.2452 - val_mse: 1.2452 - val_accuracy: 0.1738\n",
            "Epoch 105/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2903 - mse: 0.2903 - accuracy: 0.1900 - val_loss: 1.2100 - val_mse: 1.2100 - val_accuracy: 0.1734\n",
            "Epoch 106/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2758 - mse: 0.2758 - accuracy: 0.1892 - val_loss: 1.1953 - val_mse: 1.1953 - val_accuracy: 0.1749\n",
            "Epoch 107/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2833 - mse: 0.2833 - accuracy: 0.1900 - val_loss: 1.2044 - val_mse: 1.2044 - val_accuracy: 0.1752\n",
            "Epoch 108/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2772 - mse: 0.2772 - accuracy: 0.1911 - val_loss: 1.2225 - val_mse: 1.2225 - val_accuracy: 0.1773\n",
            "Epoch 109/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2943 - mse: 0.2943 - accuracy: 0.1902 - val_loss: 1.2645 - val_mse: 1.2645 - val_accuracy: 0.1775\n",
            "Epoch 110/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2856 - mse: 0.2856 - accuracy: 0.1902 - val_loss: 1.2474 - val_mse: 1.2474 - val_accuracy: 0.1760\n",
            "Epoch 111/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2771 - mse: 0.2771 - accuracy: 0.1908 - val_loss: 1.1905 - val_mse: 1.1905 - val_accuracy: 0.1754\n",
            "Epoch 112/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2799 - mse: 0.2799 - accuracy: 0.1902 - val_loss: 1.2454 - val_mse: 1.2454 - val_accuracy: 0.1804\n",
            "Epoch 113/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2697 - mse: 0.2697 - accuracy: 0.1903 - val_loss: 1.2239 - val_mse: 1.2239 - val_accuracy: 0.1733\n",
            "Epoch 114/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2777 - mse: 0.2777 - accuracy: 0.1905 - val_loss: 1.2280 - val_mse: 1.2280 - val_accuracy: 0.1711\n",
            "Epoch 115/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2781 - mse: 0.2781 - accuracy: 0.1905 - val_loss: 1.3030 - val_mse: 1.3030 - val_accuracy: 0.1731\n",
            "Epoch 116/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2550 - mse: 0.2550 - accuracy: 0.1911 - val_loss: 1.2234 - val_mse: 1.2234 - val_accuracy: 0.1755\n",
            "Epoch 117/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2742 - mse: 0.2742 - accuracy: 0.1910 - val_loss: 1.2576 - val_mse: 1.2576 - val_accuracy: 0.1778\n",
            "Epoch 118/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2722 - mse: 0.2722 - accuracy: 0.1910 - val_loss: 1.1774 - val_mse: 1.1774 - val_accuracy: 0.1745\n",
            "Epoch 119/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2702 - mse: 0.2702 - accuracy: 0.1912 - val_loss: 1.2554 - val_mse: 1.2554 - val_accuracy: 0.1715\n",
            "Epoch 120/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2525 - mse: 0.2525 - accuracy: 0.1916 - val_loss: 1.2907 - val_mse: 1.2907 - val_accuracy: 0.1693\n",
            "Epoch 121/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2629 - mse: 0.2629 - accuracy: 0.1908 - val_loss: 1.2230 - val_mse: 1.2230 - val_accuracy: 0.1724\n",
            "Epoch 122/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2519 - mse: 0.2519 - accuracy: 0.1912 - val_loss: 1.2967 - val_mse: 1.2967 - val_accuracy: 0.1752\n",
            "Epoch 123/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2570 - mse: 0.2570 - accuracy: 0.1913 - val_loss: 1.2255 - val_mse: 1.2255 - val_accuracy: 0.1772\n",
            "Epoch 124/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2748 - mse: 0.2748 - accuracy: 0.1903 - val_loss: 1.2087 - val_mse: 1.2087 - val_accuracy: 0.1785\n",
            "Epoch 125/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2444 - mse: 0.2444 - accuracy: 0.1920 - val_loss: 1.3376 - val_mse: 1.3376 - val_accuracy: 0.1741\n",
            "Epoch 126/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2324 - mse: 0.2324 - accuracy: 0.1924 - val_loss: 1.3643 - val_mse: 1.3643 - val_accuracy: 0.1697\n",
            "Epoch 127/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2601 - mse: 0.2601 - accuracy: 0.1917 - val_loss: 1.2290 - val_mse: 1.2290 - val_accuracy: 0.1785\n",
            "Epoch 128/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2453 - mse: 0.2453 - accuracy: 0.1912 - val_loss: 1.2104 - val_mse: 1.2104 - val_accuracy: 0.1731\n",
            "Epoch 129/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2407 - mse: 0.2407 - accuracy: 0.1923 - val_loss: 1.2377 - val_mse: 1.2377 - val_accuracy: 0.1767\n",
            "Epoch 130/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2473 - mse: 0.2473 - accuracy: 0.1922 - val_loss: 1.3950 - val_mse: 1.3950 - val_accuracy: 0.1796\n",
            "Epoch 131/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2485 - mse: 0.2485 - accuracy: 0.1920 - val_loss: 1.2141 - val_mse: 1.2141 - val_accuracy: 0.1749\n",
            "Epoch 132/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2416 - mse: 0.2416 - accuracy: 0.1919 - val_loss: 1.2256 - val_mse: 1.2256 - val_accuracy: 0.1733\n",
            "Epoch 133/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2494 - mse: 0.2494 - accuracy: 0.1917 - val_loss: 1.3200 - val_mse: 1.3200 - val_accuracy: 0.1798\n",
            "Epoch 134/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2429 - mse: 0.2429 - accuracy: 0.1919 - val_loss: 1.2459 - val_mse: 1.2459 - val_accuracy: 0.1728\n",
            "Epoch 135/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2367 - mse: 0.2367 - accuracy: 0.1920 - val_loss: 1.2524 - val_mse: 1.2524 - val_accuracy: 0.1752\n",
            "Epoch 136/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2411 - mse: 0.2411 - accuracy: 0.1918 - val_loss: 1.2435 - val_mse: 1.2435 - val_accuracy: 0.1770\n",
            "Epoch 137/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2285 - mse: 0.2285 - accuracy: 0.1922 - val_loss: 1.2374 - val_mse: 1.2374 - val_accuracy: 0.1752\n",
            "Epoch 138/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2153 - mse: 0.2153 - accuracy: 0.1929 - val_loss: 1.2527 - val_mse: 1.2527 - val_accuracy: 0.1753\n",
            "Epoch 139/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2367 - mse: 0.2367 - accuracy: 0.1924 - val_loss: 1.2714 - val_mse: 1.2714 - val_accuracy: 0.1772\n",
            "Epoch 140/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2250 - mse: 0.2250 - accuracy: 0.1925 - val_loss: 1.2341 - val_mse: 1.2341 - val_accuracy: 0.1780\n",
            "Epoch 141/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2357 - mse: 0.2357 - accuracy: 0.1924 - val_loss: 1.2241 - val_mse: 1.2241 - val_accuracy: 0.1757\n",
            "Epoch 142/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2255 - mse: 0.2255 - accuracy: 0.1930 - val_loss: 1.2495 - val_mse: 1.2495 - val_accuracy: 0.1771\n",
            "Epoch 143/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2170 - mse: 0.2170 - accuracy: 0.1929 - val_loss: 1.2938 - val_mse: 1.2938 - val_accuracy: 0.1732\n",
            "Epoch 144/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2240 - mse: 0.2240 - accuracy: 0.1933 - val_loss: 1.2530 - val_mse: 1.2530 - val_accuracy: 0.1792\n",
            "Epoch 145/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2297 - mse: 0.2297 - accuracy: 0.1926 - val_loss: 1.3150 - val_mse: 1.3150 - val_accuracy: 0.1806\n",
            "Epoch 146/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2139 - mse: 0.2139 - accuracy: 0.1929 - val_loss: 1.2379 - val_mse: 1.2379 - val_accuracy: 0.1797\n",
            "Epoch 147/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2100 - mse: 0.2100 - accuracy: 0.1934 - val_loss: 1.2397 - val_mse: 1.2397 - val_accuracy: 0.1765\n",
            "Epoch 148/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2238 - mse: 0.2238 - accuracy: 0.1932 - val_loss: 1.3030 - val_mse: 1.3030 - val_accuracy: 0.1765\n",
            "Epoch 149/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2166 - mse: 0.2166 - accuracy: 0.1930 - val_loss: 1.2524 - val_mse: 1.2524 - val_accuracy: 0.1779\n",
            "Epoch 150/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2186 - mse: 0.2186 - accuracy: 0.1935 - val_loss: 1.2137 - val_mse: 1.2137 - val_accuracy: 0.1770\n",
            "Epoch 151/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2115 - mse: 0.2115 - accuracy: 0.1937 - val_loss: 1.2609 - val_mse: 1.2609 - val_accuracy: 0.1757\n",
            "Epoch 152/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2376 - mse: 0.2376 - accuracy: 0.1925 - val_loss: 1.2448 - val_mse: 1.2448 - val_accuracy: 0.1814\n",
            "Epoch 153/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2117 - mse: 0.2117 - accuracy: 0.1929 - val_loss: 1.2346 - val_mse: 1.2346 - val_accuracy: 0.1795\n",
            "Epoch 154/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2195 - mse: 0.2195 - accuracy: 0.1934 - val_loss: 1.3867 - val_mse: 1.3867 - val_accuracy: 0.1710\n",
            "Epoch 155/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2066 - mse: 0.2066 - accuracy: 0.1940 - val_loss: 1.2524 - val_mse: 1.2524 - val_accuracy: 0.1786\n",
            "Epoch 156/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2317 - mse: 0.2317 - accuracy: 0.1929 - val_loss: 1.3638 - val_mse: 1.3638 - val_accuracy: 0.1735\n",
            "Epoch 157/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2170 - mse: 0.2170 - accuracy: 0.1930 - val_loss: 1.2299 - val_mse: 1.2299 - val_accuracy: 0.1777\n",
            "Epoch 158/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2131 - mse: 0.2131 - accuracy: 0.1932 - val_loss: 1.2535 - val_mse: 1.2535 - val_accuracy: 0.1798\n",
            "Epoch 159/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2059 - mse: 0.2059 - accuracy: 0.1939 - val_loss: 1.3473 - val_mse: 1.3473 - val_accuracy: 0.1679\n",
            "Epoch 160/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2102 - mse: 0.2102 - accuracy: 0.1935 - val_loss: 1.2801 - val_mse: 1.2801 - val_accuracy: 0.1723\n",
            "Epoch 161/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2006 - mse: 0.2006 - accuracy: 0.1940 - val_loss: 1.2492 - val_mse: 1.2492 - val_accuracy: 0.1752\n",
            "Epoch 162/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2228 - mse: 0.2228 - accuracy: 0.1930 - val_loss: 1.2389 - val_mse: 1.2389 - val_accuracy: 0.1757\n",
            "Epoch 163/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1964 - mse: 0.1964 - accuracy: 0.1938 - val_loss: 1.4224 - val_mse: 1.4224 - val_accuracy: 0.1677\n",
            "Epoch 164/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2203 - mse: 0.2203 - accuracy: 0.1931 - val_loss: 1.2912 - val_mse: 1.2912 - val_accuracy: 0.1778\n",
            "Epoch 165/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1930 - mse: 0.1930 - accuracy: 0.1939 - val_loss: 1.2592 - val_mse: 1.2592 - val_accuracy: 0.1767\n",
            "Epoch 166/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2039 - mse: 0.2039 - accuracy: 0.1938 - val_loss: 1.2539 - val_mse: 1.2539 - val_accuracy: 0.1784\n",
            "Epoch 167/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1986 - mse: 0.1986 - accuracy: 0.1937 - val_loss: 1.2678 - val_mse: 1.2678 - val_accuracy: 0.1829\n",
            "Epoch 168/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1956 - mse: 0.1956 - accuracy: 0.1940 - val_loss: 1.3312 - val_mse: 1.3312 - val_accuracy: 0.1737\n",
            "Epoch 169/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2013 - mse: 0.2013 - accuracy: 0.1939 - val_loss: 1.2101 - val_mse: 1.2101 - val_accuracy: 0.1799\n",
            "Epoch 170/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2058 - mse: 0.2058 - accuracy: 0.1936 - val_loss: 1.2519 - val_mse: 1.2519 - val_accuracy: 0.1789\n",
            "Epoch 171/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1985 - mse: 0.1985 - accuracy: 0.1935 - val_loss: 1.2659 - val_mse: 1.2659 - val_accuracy: 0.1763\n",
            "Epoch 172/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1852 - mse: 0.1852 - accuracy: 0.1943 - val_loss: 1.2961 - val_mse: 1.2961 - val_accuracy: 0.1802\n",
            "Epoch 173/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1908 - mse: 0.1908 - accuracy: 0.1941 - val_loss: 1.2483 - val_mse: 1.2483 - val_accuracy: 0.1819\n",
            "Epoch 174/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2021 - mse: 0.2021 - accuracy: 0.1937 - val_loss: 1.2346 - val_mse: 1.2346 - val_accuracy: 0.1756\n",
            "Epoch 175/200\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.1886 - mse: 0.1886 - accuracy: 0.1940 - val_loss: 1.2680 - val_mse: 1.2680 - val_accuracy: 0.1784\n",
            "Epoch 176/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1913 - mse: 0.1913 - accuracy: 0.1940 - val_loss: 1.2460 - val_mse: 1.2460 - val_accuracy: 0.1817\n",
            "Epoch 177/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1836 - mse: 0.1836 - accuracy: 0.1949 - val_loss: 1.2488 - val_mse: 1.2488 - val_accuracy: 0.1794\n",
            "Epoch 178/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1971 - mse: 0.1971 - accuracy: 0.1937 - val_loss: 1.2635 - val_mse: 1.2635 - val_accuracy: 0.1782\n",
            "Epoch 179/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1741 - mse: 0.1741 - accuracy: 0.1951 - val_loss: 1.2763 - val_mse: 1.2763 - val_accuracy: 0.1764\n",
            "Epoch 180/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1922 - mse: 0.1922 - accuracy: 0.1942 - val_loss: 1.2583 - val_mse: 1.2583 - val_accuracy: 0.1750\n",
            "Epoch 181/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1989 - mse: 0.1989 - accuracy: 0.1937 - val_loss: 1.2807 - val_mse: 1.2807 - val_accuracy: 0.1742\n",
            "Epoch 182/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1708 - mse: 0.1708 - accuracy: 0.1946 - val_loss: 1.2866 - val_mse: 1.2866 - val_accuracy: 0.1776\n",
            "Epoch 183/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1869 - mse: 0.1869 - accuracy: 0.1941 - val_loss: 1.2883 - val_mse: 1.2883 - val_accuracy: 0.1795\n",
            "Epoch 184/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1691 - mse: 0.1691 - accuracy: 0.1954 - val_loss: 1.3211 - val_mse: 1.3211 - val_accuracy: 0.1745\n",
            "Epoch 185/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1880 - mse: 0.1880 - accuracy: 0.1943 - val_loss: 1.2654 - val_mse: 1.2654 - val_accuracy: 0.1768\n",
            "Epoch 186/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1957 - mse: 0.1957 - accuracy: 0.1943 - val_loss: 1.2602 - val_mse: 1.2602 - val_accuracy: 0.1773\n",
            "Epoch 187/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1651 - mse: 0.1651 - accuracy: 0.1949 - val_loss: 1.2858 - val_mse: 1.2858 - val_accuracy: 0.1786\n",
            "Epoch 188/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1947 - mse: 0.1947 - accuracy: 0.1942 - val_loss: 1.2298 - val_mse: 1.2298 - val_accuracy: 0.1765\n",
            "Epoch 189/200\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1567 - mse: 0.1567 - accuracy: 0.1954 - val_loss: 1.2515 - val_mse: 1.2515 - val_accuracy: 0.1779\n",
            "Epoch 190/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1859 - mse: 0.1859 - accuracy: 0.1942 - val_loss: 1.4120 - val_mse: 1.4120 - val_accuracy: 0.1753\n",
            "Epoch 191/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1771 - mse: 0.1771 - accuracy: 0.1947 - val_loss: 1.2696 - val_mse: 1.2696 - val_accuracy: 0.1798\n",
            "Epoch 192/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1709 - mse: 0.1709 - accuracy: 0.1950 - val_loss: 1.2694 - val_mse: 1.2694 - val_accuracy: 0.1763\n",
            "Epoch 193/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1783 - mse: 0.1783 - accuracy: 0.1946 - val_loss: 1.2586 - val_mse: 1.2586 - val_accuracy: 0.1795\n",
            "Epoch 194/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1801 - mse: 0.1801 - accuracy: 0.1946 - val_loss: 1.2541 - val_mse: 1.2541 - val_accuracy: 0.1785\n",
            "Epoch 195/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1744 - mse: 0.1744 - accuracy: 0.1949 - val_loss: 1.2865 - val_mse: 1.2865 - val_accuracy: 0.1775\n",
            "Epoch 196/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1739 - mse: 0.1739 - accuracy: 0.1946 - val_loss: 1.3091 - val_mse: 1.3091 - val_accuracy: 0.1761\n",
            "Epoch 197/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1802 - mse: 0.1802 - accuracy: 0.1942 - val_loss: 1.3302 - val_mse: 1.3302 - val_accuracy: 0.1793\n",
            "Epoch 198/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1679 - mse: 0.1679 - accuracy: 0.1948 - val_loss: 1.2701 - val_mse: 1.2701 - val_accuracy: 0.1741\n",
            "Epoch 199/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1731 - mse: 0.1731 - accuracy: 0.1950 - val_loss: 1.2841 - val_mse: 1.2841 - val_accuracy: 0.1742\n",
            "Epoch 200/200\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1691 - mse: 0.1691 - accuracy: 0.1951 - val_loss: 1.2635 - val_mse: 1.2635 - val_accuracy: 0.1777\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f395eeaa390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "popLH5MNl9oK"
      },
      "source": [
        "We are able to get **19.51%** accuracy on the Test set and **17.77%** accuracy on Training set on even 200 epochs when we use **single unit** relu function as the activation function. This shows **we can't solve classification problems with regression** and hence, getting such a low accuracy.\n",
        "\n"
      ]
    }
  ]
}