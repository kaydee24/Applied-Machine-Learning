# -*- coding: utf-8 -*-
"""Handwriting recognition LAB7 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1orTXJSv82V9r3fwyLKw1uwgAwkJkYJ_-

This is an important assignment.

Take some time to undrstand the classic MNIST dataset here.
http://yann.lecun.com/exdb/mnist/

In this, we will use the copy of the MNIST dataset already provided in tensorflow package in an easy to access form. But we will not use tensorflow for designing the neural network. We will use that from next assignment. In this assisgnment, we will program a neural network from scratch.
"""

import tensorflow as tf; 
import numpy as np
import matplotlib.pyplot as plt
import random

mnist = tf.keras.datasets.mnist;

(x_train, y_train), (x_test, y_test) = mnist.load_data()
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

x_train = tf.keras.utils.normalize(x_train)
x_test = tf.keras.utils.normalize(x_test)

#Reshape all images into single rows
X = x_train.reshape(60000,784)
y = y_train;

#Let's look at the image that corresponds to the first row of X
plt.imshow(x_train[0], cmap = plt.cm.binary)

a= X.shape

input_layer_size  = a[1];  # 28x28 Input Images of Digits
hidden_layer_size = 25;   # 25 hidden units
num_labels = 10;          # 10 labels, from 0 to 9
lambd = 2;
alpha = 2;

theta1 = np.zeros([input_layer_size+1, hidden_layer_size]);
theta2 = np.zeros([hidden_layer_size+1, num_labels]);

epsilon_init = 0.1;
theta1 = np.random.uniform(low=0, high=1, size=(input_layer_size+1, hidden_layer_size)) * 2 * epsilon_init - epsilon_init;

epsilon_init = 0.1;
theta2 = np.random.uniform(low=0, high=1, size=(hidden_layer_size+1, num_labels)) * 2 * epsilon_init - epsilon_init;

def sigmoid(x):
  return 1/(1 + np.exp(-x))

def sigmoidGradient(z):
  s  = 1/(1+np.exp(-z));
  return s*(1-s);

Z1 = np.dot(np.concatenate((np.ones([a[0],1]), X), axis = 1),theta1);

first_Layer_Output = sigmoid(Z1);

a= first_Layer_Output.shape
Z2 = np.dot(np.concatenate((np.ones([a[0],1]), first_Layer_Output), axis = 1),theta2);
second_Layer_Output = sigmoid(Z2);
second_Layer_Output.shape

YMat = np.zeros(second_Layer_Output.shape);
a = second_Layer_Output.shape;

YMat.shape

for i in range(0,a[0]):
    YMat[i, y[i]] = 1;



"""**Assignment 7.1 (2 marks)**

Comment the following blocks of code in detail
"""

#Question 7.1

no_of_Interations = 100;  # No. of steps taken towards the minimun in gradient desent 
J = np.zeros([no_of_Interations,1]);  #Cost Function Matrix (Value of cost function is stored of every iterations )

for i in range(0,no_of_Interations):   
  a= X.shape;                        # Size of the Input data (60000,784)
  Z1 = np.dot(np.concatenate((np.ones([a[0],1]), X), axis = 1),theta1);  # It is the value of each nodes in the first layer by multiplying all 784 input pts to 25 weight values for each nodes(A0=a0*w01+a1*w11...) 
  first_Layer_Output = sigmoid(Z1);  # It is the sigmoid of each nodes, which is the final output of the first layer

  a= first_Layer_Output.shape   # Size of the output of first layer
  Z2 = np.dot(np.concatenate((np.ones([a[0],1]), first_Layer_Output), axis = 1),theta2); # It is the value of each nodes in the second layer by multiplying all 25 data pts to 10 weight values for each nodes(A0=a0*w02+a1*w12...)
  second_Layer_Output = sigmoid(Z2);    # It is the sigmoid of each nodes, which is the final output of the second layer
  
  J[i] = (-1/a[1])*np.sum(YMat*np.log(second_Layer_Output) + (1-YMat)*np.log(1-second_Layer_Output)) + (lambd/(2*a[1]))*np.sum(theta1[1:,:]*theta1[1:,:]) + (lambd/(2*a[1]))*np.sum(theta2[1:,:]*theta2[1:,:]); # value of cost function at ith iteration

  da2dz2 = (second_Layer_Output - YMat);           #derivative of a2 wrt z2
  da1dz1 = da2dz2.dot(np.transpose(theta2[1:,:]))*sigmoidGradient(Z1);  # derivative of a1 wrt z1
  
  b = second_Layer_Output.shape;       # Size of the output of second/final layer
  Theta2_grad = np.dot(np.transpose(np.concatenate((np.ones([a[0],1]), first_Layer_Output),1)),da2dz2);    
  Theta2_grad = Theta2_grad +lambd*np.concatenate((np.zeros([1, b[1]]), theta2[1:,:]),0);        #Actual gradient of theta2
  
  Theta1_grad = np.dot(np.transpose(np.concatenate((np.ones([a[0],1]), X),1)),da1dz1);           
  Theta1_grad = Theta1_grad + lambd*np.concatenate((np.zeros([1, a[1]]), theta1[1:,:]),0);       #Actual gradient of theta1
  
  Theta2_grad = Theta2_grad/a[0];                            #Dividing the theta 2 gradient by a[0]
  Theta1_grad = Theta1_grad/a[0];                            #Dividing the theta 1 gradient by a[0] 
  
  theta1 = theta1- alpha*Theta1_grad;                        #Updated value of theta 1
  theta2 = theta2 - alpha*Theta2_grad;                       #Updated value of theta 2

  if(i%10 == 0):                                            #ploting after every 10 iterations
    plt.plot(J[0:i]);
    plt.title('Cost function reduction for lambda = '+ str(lambd)+ ' and alpha = '+ str(alpha));
    plt.xlabel('Iteration number');
    plt.ylabel('Cost function (J\theta)');
    plt.show();
    plt.pause(0.05);

#Evaluation of training set

a= X.shape;
Z1 = np.dot(np.concatenate((np.ones([a[0],1]), X), axis = 1),theta1);
first_Layer_Output = sigmoid(Z1);

a= first_Layer_Output.shape
Z2 = np.dot(np.concatenate((np.ones([a[0],1]), first_Layer_Output), axis = 1),theta2);
second_Layer_Output = sigmoid(Z2);

I = np.zeros(y.shape);
training_Error_Percentage = 0;
for i in range(0,a[0]):
    I[i] = np.argmax(second_Layer_Output[i,:]);
    if(I[i] == y[i]):
      training_Error_Percentage = training_Error_Percentage+1;
training_Error_Percentage = 100*training_Error_Percentage/a[0];
print(training_Error_Percentage)

"""The Training accuracy is **84.69%**"""

#Evaluation of test set
X_test = x_test.reshape(10000,784);

a= X_test.shape;
Z1 = np.dot(np.concatenate((np.ones([a[0],1]), X_test), axis = 1),theta1);
first_Layer_Output = sigmoid(Z1);

a= first_Layer_Output.shape
Z2 = np.dot(np.concatenate((np.ones([a[0],1]), first_Layer_Output), axis = 1),theta2);
second_Layer_Output = sigmoid(Z2);

I = np.zeros(y_test.shape);
test_Error_Percentage = 0;
for i in range(0,a[0]):
    I[i] = np.argmax(second_Layer_Output[i,:]);
    if(I[i] == y_test[i]):
      test_Error_Percentage = test_Error_Percentage+1;
test_Error_Percentage = 100*test_Error_Percentage/a[0];
print(test_Error_Percentage)

"""The Test accuracy is **84.85%**

**Assignment 7.2 (1 mark)**
Show any 20 images in a grid and set their title as "Actual number/Predicted number"
"""

#Question 7.2

fig=plt.figure(figsize=(8, 8))
columns = 4
rows = 5
ax = []

for i in range(columns*rows):
    img = x_test[i]
    ax.append( fig.add_subplot(rows, columns, i+1) )
    ax[-1].set_title("%d / %d" %(y_test[i],I[i]))  
    plt.imshow(img)
plt.tight_layout(True)
plt.show()

#On top of every image, the right hand side is the actual number and the shown number is predicted number.